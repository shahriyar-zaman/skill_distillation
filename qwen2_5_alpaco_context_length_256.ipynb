{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fe983dc931c44e37b010cdb4d63ce679": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b1d25082672f4b488e0acf9fb6103148",
              "IPY_MODEL_03f0770c56d147809e37c747a06f6409",
              "IPY_MODEL_727243857825456d99a66af6a950bcaf"
            ],
            "layout": "IPY_MODEL_5b28cccaaebd42da87e8133abde1bf44"
          }
        },
        "b1d25082672f4b488e0acf9fb6103148": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b05b01b01ece493abe3f3f833eae5fa0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_710a5dfa450a4663972c59ca08a2e753",
            "value": "Map:â€‡100%"
          }
        },
        "03f0770c56d147809e37c747a06f6409": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_870fbc4470ac4144a60a34aee772690f",
            "max": 46801,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_065e2bd8319a42b1a17518c9a10533a9",
            "value": 46801
          }
        },
        "727243857825456d99a66af6a950bcaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6ab7f2d1da945a0a8b2064a4ca2cca5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_23ac5fbbaed5470d93642d9647498405",
            "value": "â€‡46801/46801â€‡[00:37&lt;00:00,â€‡1496.39â€‡examples/s]"
          }
        },
        "5b28cccaaebd42da87e8133abde1bf44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b05b01b01ece493abe3f3f833eae5fa0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "710a5dfa450a4663972c59ca08a2e753": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "870fbc4470ac4144a60a34aee772690f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "065e2bd8319a42b1a17518c9a10533a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b6ab7f2d1da945a0a8b2064a4ca2cca5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23ac5fbbaed5470d93642d9647498405": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4761621492c4473b55ea0e79796bfdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf2632245556452cb7a1ec02a96d376b",
              "IPY_MODEL_baae95bc4c4846e7b06bbf8a68378d02",
              "IPY_MODEL_abcf4faed7094fe592a4aa3c4e39c5b8"
            ],
            "layout": "IPY_MODEL_9140e3ec4e29457aa3b9b0ad3c8f5335"
          }
        },
        "cf2632245556452cb7a1ec02a96d376b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfc8c3deea9b4972a27260819b29551f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7fa4c225957a43948acbdbe8fa4fb5fa",
            "value": "Map:â€‡100%"
          }
        },
        "baae95bc4c4846e7b06bbf8a68378d02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4782e431dc654dd69cd1ee32d25d08e8",
            "max": 5201,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d596429254714062b5a3c6f1cb1bf81a",
            "value": 5201
          }
        },
        "abcf4faed7094fe592a4aa3c4e39c5b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b79ca99a1b8746e0afdcc3a220f6519a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_81ae7421a9f7401e9f1311b92d0b7b48",
            "value": "â€‡5201/5201â€‡[00:03&lt;00:00,â€‡1435.55â€‡examples/s]"
          }
        },
        "9140e3ec4e29457aa3b9b0ad3c8f5335": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfc8c3deea9b4972a27260819b29551f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fa4c225957a43948acbdbe8fa4fb5fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4782e431dc654dd69cd1ee32d25d08e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d596429254714062b5a3c6f1cb1bf81a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b79ca99a1b8746e0afdcc3a220f6519a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81ae7421a9f7401e9f1311b92d0b7b48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "%%capture\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install evaluate  # Install the evaluate library for metrics\n",
        "!pip install rouge_score\n",
        "\n",
        "# Import libraries\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "import math\n",
        "from datasets import load_dataset\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq, Trainer\n",
        "from unsloth import is_bfloat16_supported\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import evaluate\n",
        "\n",
        "# Initialize metrics\n",
        "bleu_metric = evaluate.load(\"bleu\")\n",
        "rouge_metric = evaluate.load(\"rouge\")"
      ],
      "metadata": {
        "id": "RbQmnJ4EOUiI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 256  # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# Load the student model and tokenizer (Qwen 2.5 0.5B)\n",
        "student_model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Qwen2.5-0.5B-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")"
      ],
      "metadata": {
        "id": "7mhWZhx5OVJI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6247343f-db72-4b97-c221-0dbb26ac793f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2024.10.7: Fast Qwen2 patching. Transformers = 4.44.2.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.5.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the student model for PEFT\n",
        "student_model = FastLanguageModel.get_peft_model(\n",
        "    student_model,\n",
        "    r=16,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,  # Supports any, but = 0 is optimized\n",
        "    bias=\"none\",     # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
        "    random_state=3407,\n",
        "    use_rslora=False,   # We support rank stabilized LoRA\n",
        "    loftq_config=None,  # And LoftQ\n",
        ")\n",
        "\n",
        "# Load the teacher model (Qwen 2.5 1.5B)\n",
        "teacher_model, _ = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Qwen2.5-1.5B-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "teacher_model.eval()\n",
        "# teacher_model.to(\"cuda\")  # Ensure the teacher model is on the GPU"
      ],
      "metadata": {
        "id": "qPIL2dVHOayJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5ae6663-a6ad-4825-f46f-edf78e1dfcb5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2024.10.7: Fast Qwen2 patching. Transformers = 4.44.2.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.5.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Qwen2ForCausalLM(\n",
              "  (model): Qwen2Model(\n",
              "    (embed_tokens): Embedding(151936, 1536)\n",
              "    (layers): ModuleList(\n",
              "      (0-27): 28 x Qwen2DecoderLayer(\n",
              "        (self_attn): Qwen2Attention(\n",
              "          (q_proj): Linear4bit(in_features=1536, out_features=1536, bias=True)\n",
              "          (k_proj): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
              "          (v_proj): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
              "          (o_proj): Linear4bit(in_features=1536, out_features=1536, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): Qwen2MLP(\n",
              "          (gate_proj): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=8960, out_features=1536, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the chat template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"qwen-2.5\",\n",
        ")\n",
        "\n",
        "# Function to format prompts\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "\n",
        "    # Combine instruction and input for the prompt\n",
        "    convos = []\n",
        "    for instruction, input_text in zip(instructions, inputs):\n",
        "        if input_text.strip() != '':\n",
        "            content = f\"Instruction: {instruction}\\nInput: {input_text}\"\n",
        "        else:\n",
        "            content = f\"Instruction: {instruction}\"\n",
        "        convos.append({\"role\": \"user\", \"content\": content})\n",
        "\n",
        "    responses = [{\"role\": \"assistant\", \"content\": output} for output in outputs]\n",
        "\n",
        "    # Combine conversations and apply the chat template\n",
        "    conversations = [{\"conversations\": [convo, response]} for convo, response in zip(convos, responses)]\n",
        "\n",
        "    texts = [tokenizer.apply_chat_template(convo[\"conversations\"], tokenize=False, add_generation_prompt=False)\n",
        "             for convo in conversations]\n",
        "\n",
        "    target_texts = outputs  # Keep the outputs as target texts\n",
        "\n",
        "    return {\"text\": texts, \"target_text\": target_texts}\n",
        "\n",
        "# Load and split the dataset\n",
        "dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")"
      ],
      "metadata": {
        "id": "8ImxpbHIOhpZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into training and validation sets\n",
        "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = dataset['train']\n",
        "eval_dataset = dataset['test']\n",
        "\n",
        "# Apply formatting to both datasets\n",
        "train_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n",
        "eval_dataset = eval_dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# Tokenize the datasets and prepare labels\n",
        "def tokenize_function(examples):\n",
        "    tokenized = tokenizer(examples[\"text\"], truncation=True, max_length=max_seq_length, padding=\"max_length\")\n",
        "    labels = tokenized['input_ids'].copy()\n",
        "    # Set padding tokens to -100 to ignore them in the loss\n",
        "    labels = [[(label if label != tokenizer.pad_token_id else -100) for label in label_list] for label_list in labels]\n",
        "    tokenized['labels'] = labels\n",
        "    tokenized['target_text'] = examples['target_text']\n",
        "    return tokenized\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Set the format to PyTorch tensors\n",
        "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels', 'target_text'])\n",
        "eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels', 'target_text'])"
      ],
      "metadata": {
        "id": "wKYd4GmsOl5y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "fe983dc931c44e37b010cdb4d63ce679",
            "b1d25082672f4b488e0acf9fb6103148",
            "03f0770c56d147809e37c747a06f6409",
            "727243857825456d99a66af6a950bcaf",
            "5b28cccaaebd42da87e8133abde1bf44",
            "b05b01b01ece493abe3f3f833eae5fa0",
            "710a5dfa450a4663972c59ca08a2e753",
            "870fbc4470ac4144a60a34aee772690f",
            "065e2bd8319a42b1a17518c9a10533a9",
            "b6ab7f2d1da945a0a8b2064a4ca2cca5",
            "23ac5fbbaed5470d93642d9647498405",
            "a4761621492c4473b55ea0e79796bfdd",
            "cf2632245556452cb7a1ec02a96d376b",
            "baae95bc4c4846e7b06bbf8a68378d02",
            "abcf4faed7094fe592a4aa3c4e39c5b8",
            "9140e3ec4e29457aa3b9b0ad3c8f5335",
            "bfc8c3deea9b4972a27260819b29551f",
            "7fa4c225957a43948acbdbe8fa4fb5fa",
            "4782e431dc654dd69cd1ee32d25d08e8",
            "d596429254714062b5a3c6f1cb1bf81a",
            "b79ca99a1b8746e0afdcc3a220f6519a",
            "81ae7421a9f7401e9f1311b92d0b7b48"
          ]
        },
        "outputId": "ca08ecb7-0e8c-4023-e02c-88c62268f606"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/46801 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe983dc931c44e37b010cdb4d63ce679"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5201 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4761621492c4473b55ea0e79796bfdd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Knowledge Distillation Trainer\n",
        "class KDTrainer(Trainer):\n",
        "    def __init__(self, teacher_model, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.teacher_model = teacher_model\n",
        "        self.teacher_model.eval()\n",
        "        # No need to move teacher_model to 'cuda' manually\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        # Move inputs to the student's device\n",
        "        input_ids = inputs['input_ids'].to(model.device)\n",
        "        attention_mask = inputs['attention_mask'].to(model.device)\n",
        "        labels = inputs['labels'].to(model.device)\n",
        "\n",
        "        # Get student outputs\n",
        "        student_outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        student_logits = student_outputs.logits  # shape: [batch_size, seq_length, vocab_size]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Get the device of the teacher model\n",
        "            teacher_device = next(self.teacher_model.parameters()).device\n",
        "            # Move inputs to the teacher's device\n",
        "            input_ids_teacher = inputs['input_ids'].to(teacher_device)\n",
        "            attention_mask_teacher = inputs['attention_mask'].to(teacher_device)\n",
        "\n",
        "            # Get teacher outputs\n",
        "            teacher_outputs = self.teacher_model(input_ids=input_ids_teacher, attention_mask=attention_mask_teacher)\n",
        "            teacher_logits = teacher_outputs.logits  # shape: [batch_size, seq_length, vocab_size]\n",
        "\n",
        "        # Set temperature and alpha\n",
        "        temperature = 2.0\n",
        "        alpha = 0.5\n",
        "\n",
        "        # Compute student cross-entropy loss\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
        "        student_ce_loss = loss_fct(student_logits.view(-1, student_logits.size(-1)), labels.view(-1))\n",
        "\n",
        "        # Compute KL divergence loss at each token position\n",
        "        student_log_probs = torch.nn.functional.log_softmax(student_logits / temperature, dim=-1)\n",
        "        teacher_probs = torch.nn.functional.softmax(teacher_logits / temperature, dim=-1)\n",
        "        kl_loss_fct = torch.nn.KLDivLoss(reduction='none')\n",
        "        kl_div_loss = kl_loss_fct(student_log_probs, teacher_probs)  # shape: [batch_size, seq_length, vocab_size]\n",
        "        # Sum over vocab dimension\n",
        "        kl_div_loss = kl_div_loss.sum(-1)  # shape: [batch_size, seq_length]\n",
        "        # Mask out padding tokens\n",
        "        mask = (labels != -100).float()\n",
        "        kl_div_loss = (kl_div_loss * mask).sum() / mask.sum()\n",
        "        # Adjust for temperature scaling\n",
        "        kl_div_loss = kl_div_loss * (temperature ** 2)\n",
        "\n",
        "        # Combine losses\n",
        "        loss = alpha * kl_div_loss + (1 - alpha) * student_ce_loss\n",
        "\n",
        "        return (loss, student_outputs) if return_outputs else loss\n",
        "\n",
        "    def _move_model_to_device(self, model, device):\n",
        "        # Override to prevent moving model to device\n",
        "        pass"
      ],
      "metadata": {
        "id": "QVPVSTb-OquC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set training arguments\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=1,\n",
        "    warmup_steps=5,\n",
        "    max_steps=60,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,  # Set to True for T4 GPU\n",
        "    bf16=False,  # T4 does not support bfloat16\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs\",\n",
        "    report_to=\"none\",  # Use this for WandB etc\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=10,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True, return_tensors='pt')\n",
        "\n",
        "# Initialize the KDTrainer\n",
        "trainer = KDTrainer(\n",
        "    model=student_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    teacher_model=teacher_model,\n",
        ")"
      ],
      "metadata": {
        "id": "iwEJd5y9Oul8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ebf4d22-41d5-4b71-cf3f-d728e647ce7b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "ohXJeNRnO0Lq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "4f5d7dce-563a-4160-f9bb-15861b8eca03"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 2:53:24, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>6.101100</td>\n",
              "      <td>6.157755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.711800</td>\n",
              "      <td>4.461288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>3.102800</td>\n",
              "      <td>3.602319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>3.302400</td>\n",
              "      <td>3.196372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.879200</td>\n",
              "      <td>2.963253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.739700</td>\n",
              "      <td>2.866669</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=60, training_loss=4.302190721035004, metrics={'train_runtime': 10407.3018, 'train_samples_per_second': 0.006, 'train_steps_per_second': 0.006, 'total_flos': 46340902748160.0, 'train_loss': 4.302190721035004, 'epoch': 0.0012820238883784535})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model to get perplexity\n",
        "eval_results = trainer.evaluate()\n",
        "perplexity = math.exp(eval_results[\"eval_loss\"])\n",
        "print(f\"Perplexity: {perplexity}\")\n",
        "\n",
        "# Prepare the student model for inference\n",
        "FastLanguageModel.for_inference(student_model)  # Enable native 2x faster inference"
      ],
      "metadata": {
        "id": "QmJM7s-qO4T7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ec1e3a95-19d2-4794-a1ff-188ce5cd51b0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='651' max='651' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [651/651 28:44]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 17.578365968066567\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): Qwen2ForCausalLM(\n",
              "      (model): Qwen2Model(\n",
              "        (embed_tokens): Embedding(151936, 896)\n",
              "        (layers): ModuleList(\n",
              "          (0-23): 24 x Qwen2DecoderLayer(\n",
              "            (self_attn): Qwen2Attention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=896, out_features=896, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=896, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=896, out_features=128, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=128, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=896, out_features=128, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=128, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=896, out_features=896, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=896, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen2MLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4864, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4864, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4864, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=896, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "          )\n",
              "        )\n",
              "        (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "      )\n",
              "      (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate predictions and compute metrics\n",
        "def evaluate_model(model, tokenizer, eval_dataset, batch_size=8):\n",
        "    eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size)\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(eval_dataloader):\n",
        "            input_ids = batch['input_ids'].to(model.device)\n",
        "            attention_mask = batch['attention_mask'].to(model.device)\n",
        "\n",
        "            # Generate outputs from the model\n",
        "            outputs = model.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                max_new_tokens=256,\n",
        "                temperature=1.0,\n",
        "                min_p=0.1,\n",
        "            )\n",
        "\n",
        "            # Get the generated tokens (excluding the prompt)\n",
        "            generated_tokens = outputs[:, input_ids.size(1):]\n",
        "            decoded_outputs = tokenizer.batch_decode(\n",
        "                generated_tokens, skip_special_tokens=True\n",
        "            )\n",
        "            predictions.extend(decoded_outputs)\n",
        "\n",
        "            # For evaluation, we need the reference outputs\n",
        "            references.extend(batch['target_text'])\n",
        "\n",
        "    # Compute BLEU and ROUGE scores\n",
        "    bleu_score = bleu_metric.compute(predictions=predictions, references=references)\n",
        "    rouge_score = rouge_metric.compute(predictions=predictions, references=references)\n",
        "\n",
        "    print(f\"BLEU score: {bleu_score['bleu']}\")\n",
        "    print(f\"ROUGE scores:\")\n",
        "    for key, value in rouge_score.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(student_model, tokenizer, eval_dataset)"
      ],
      "metadata": {
        "id": "c53Kw3NXO5Ks",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcb14d0e-bd4a-44c0-d0c8-462666dddf5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|â–Œ         | 38/651 [08:41<2:20:42, 13.77s/it]"
          ]
        }
      ]
    }
  ]
}